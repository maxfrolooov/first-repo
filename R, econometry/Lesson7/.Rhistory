ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, col = Species)) +
geom_point(iris, size = Petal.Length)
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, col = Species)) +
geom_point()
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, col = Species)) +
geom_point(size = Petal.Length)
iris$Petal.Length
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, col = Species, size = Petal.Length)) +
geom_point(size = Petal.Length)
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, col = Species, size = Petal.Length)) +
geom_point()
getwd()
my_mean <- mean(10^6:10^9)
write.csv(df, "df.csv")
boxplot(mpg ~ am, df, ylab = "MPG", main ="MPG and AM",
col = "green", cex.lab = 1.3, cex.axis = 1.3) # насколько различаются значения признакак по группам
save(my_mean, file = "my_mean.RData")
cov.yeps
cor(y, x, eps)
cor(matrix(y, x, eps))
cor(matrix(c(y, x, eps), ncol=3))
cor_matrix <- cor(matrix(c(y, x, eps), ncol=3))
cov.yeps
corr.yxeps
sd.yxeps
diag(cov.yeps)
corr.yxeps <- cov.yeps*(1/sd.yxeps)
corr.yxeps
corr.yxeps <- t(1/sd.yxeps)
corr.yxeps
1/sd.yxeps
corr.yxeps <- t(1/sd.yxeps)*cov.yeps
t(1/sd.yxeps)
cov.yeps
corr.yxeps <- cov.yeps*t(1/sd.yxeps)
corr.yxeps <- cov.yeps*(1/sd.yxeps)
1/sd.yxeps
diag(cov.yeps)
cov_matrix <- cov(matrix(c(y, x, eps), ncol=3))
cor_matrix
cov_matrix
cov.yeps
cov.eps <- matrix(c(17, 8, 1, 8, 4, 0, 1, 0, 1), nrow=3)
matrix(cbind(c(y, x, eps)))
cor(matrix(cbind(c(y, x, eps))))
cor(matrix(cbind(c(y, x, eps),3)))
cor(matrix(cbind(vector123,vector123,vector123)))
vector123 <- c(y, x, eps)
cor(matrix(cbind(vector123,vector123,vector123)))
cbind(vector123,vector123,vector123)
c(y, x, eps)
(matrix(c(y, x, eps), ncol=3)
cor(matrix(c(y, x, eps), ncol=3))
cor_matrix <- cor(matrix(c(y, x, eps), ncol=3))
cor_matrix
c(y, x, eps)
matrix(c(y, x, eps), ncol=3)
cor(matrix(c(y, x, eps), ncol=3))
c(y, x, eps)
corr.yxeps <- cov.yeps%*%t(1/sd.yxeps)
corr.yxeps <- t(1/sd.yxeps)%*%cov.yeps
corr.yxeps
x
c(y,x,eps)
rbind(y,x,eps)
matrix(rbind(y,x,eps))
matrix(c(y, x, eps), ncol=3)
matrix(cbind(y,x,eps))
matrix(cbind(y,x,eps), ncol = 3)
matrix(rbind(y,x,eps), ncol = 3)
matrix(cbind(y,x,eps), ncol = 3)
cov.eps[1]
cov.eps[5]
cov.eps
cov.eps[1,2]
nrow(cov.eps)
for (i in 1:nrow(cov.eps)){
for (j in 1:nrow(cov.eps)){
corr.yxeps[i,j] <- cov.eps[i,j] / sd.yxeps[i] / sd.yxeps[j]
}
}
sd.yxeps[1]
cov.eps[1,1] / sd.yxeps[1]
cov.eps[1,1] / sd.yxeps[1]
yeye <- cov.eps[1,1] / sd.yxeps[1]
yeye
sd.yxeps[1]
cov.eps[1,1]
yeye <- cov.eps[1,1] / sd.yxeps[1] / sd.yxeps[1]
yeye
for (i in 1:nrow(cov.eps)){
for (j in 1:nrow(cov.eps)){
corr.yxeps[i,j] <- cov.eps[i,j] / sd.yxeps[i] / sd.yxeps[j]
}
}
corr.yxeps[i,j] <- round(cov.eps[i,j] / sd.yxeps[i] / sd.yxeps[j], 2)
print(i)
for (i in 1:100){
print(i)
}
# corr.yxeps <- t(1/sd.yxeps)
corr.yxeps <- matrix(rep(NA, 9), ncol=3)
corr.yxeps
for (i in 1:nrow(cov.eps)){
for (j in 1:nrow(cov.eps)){
corr.yxeps[i,j] <- round(cov.eps[i,j] / sd.yxeps[i] / sd.yxeps[j], 2)
}
}
corr.yxeps
for (i in 1:nrow(cov.eps)){
for (j in 1:nrow(cov.eps)){
corr.yxeps[i,j] <- round(cov.eps[i,j] / sd.yxeps[i] / sd.yxeps[j], 3)
}
}
corr.yxeps
cor_matrix
n <- 83
set.seed(1542643545) # случайные числа, псевдослучайные
eps <- rnorm(n) # mean = 0, sd = 1
#qnorm()
#pnorm()
mean(eps)
var(eps)
x <- rnorm(n, mean=3, sd=2)
y <- -5 + 2 * x + eps
plot(y ~ x) # диаграмма
rug(x, side=3) # бахрома
rug(y, side=4)
grid() # сетка
# корреляция, paste0 - склеить без пробела, round округление
s_cor <- round(cor(x, y),4)
text(4, -4, paste0("r=", s_cor)) # текстовая надпись на диаграмме
plot(data.frame(y,x,eps)) # попарные графики
abline(v=mean(x), h = mean(y))
plot(y ~ x) # диаграмма
rug(x, side=3) # бахрома
rug(y, side=4)
grid() # сетка
# корреляция, paste0 - склеить без пробела, round округление
s_cor <- round(cor(x, y),4)
text(4, -4, paste0("r=", s_cor)) # текстовая надпись на диаграмме
abline(v=mean(x), h = mean(y))
# matrix(c(1, 4/sqrt(17), 1/sqrt(17)), ncol=3)
cor_matrix <- cor(matrix(c(y, x, eps), ncol=3)) # корреляционная матрица для выборки + ниже ковариационная
cov_matrix <- cov(matrix(c(y, x, eps), ncol=3))
cov.eps <- matrix(c(17, 8, 1, 8, 4, 0, 1, 0, 1), nrow=3) # теоритическая ковариационная матрица
sd.yxeps <- sqrt(diag(cov.yeps)) # стандартное отклонение
# corr.yxeps <- t(1/sd.yxeps)
corr.yxeps <- matrix(rep(NA, 9), ncol=3)
for (i in 1:nrow(cov.eps)){
for (j in 1:nrow(cov.eps)){
corr.yxeps[i,j] <- round(cov.eps[i,j] / sd.yxeps[i] / sd.yxeps[j], 3)
}
}
corr.yxeps
cor_matrix
cov.eps
cov_matrix
corr.yxeps
for (i in 1:nrow(cov.eps)){
for (j in 1:nrow(cov.eps)){
corr.yxeps[i,j] <- round(cov.eps[i,j] / sd.yxeps[i] / sd.yxeps[j], 3)
}
}
corr.yxeps <- t(1/sd.yxeps)
corr.yxeps <- matrix(rep(NA, 9), ncol=3)
for (i in 1:nrow(cov.eps)){
for (j in 1:nrow(cov.eps)){
corr.yxeps[i,j] <- round(cov.eps[i,j] / sd.yxeps[i] / sd.yxeps[j], 3)
}
}
corr.yxeps
cor_matrix
sd.yxeps <- sqrt(diag(cov.yeps)) # стандартное отклонение
cov.yeps <- matrix(c(17, 8, 1, 8, 4, 0, 1, 0, 1), nrow=3) # теоритическая ковариационная матрица
sd.yxeps <- sqrt(diag(cov.yeps)) # стандартное отклонение
#corr.yxeps <- t(1/sd.yxeps)
corr.yxeps <- matrix(rep(NA, 9), ncol=3)
for (i in 1:nrow(cov.eps)){
for (j in 1:nrow(cov.eps)){
corr.yxeps[i,j] <- round(cov.eps[i,j] / sd.yxeps[i] / sd.yxeps[j], 3)
}
}
corr.yxeps
cor_matrix
cov.eps
cov_matrix
t.test(y)
matrix(c(y, x, eps), ncol=3)
install.packages("Ecdat")
install.packages("Ecdat")
library(Ecdat)
data("CRSPday")
data("CRSPday")
data(CRSPday) #
data(CRSPday) #
install.packages("Ecdat")
install.packages("Ecdat")
install.packages("Ecdat")
install.packages("Ecdat")
library(Ecdat)
data(CRSPday) #
(CRSPday)
(CRSPday)
df <- data.frame(CRSPday)
View(df)
View(df)
plot.ts(CRSPday)
?plot.ts
plot(CRSPday)
plot.ts(CRSPday)
plot(CRSPday)
plot.ts(CRSPday)
plot.ts(CRSPday[,4:7])
plot.ts(df[,4:7])
rm(CRSPday)
plot.ts(df$ge)
abline(h=0, col = "red")
abline(h=mean(df$ge), col = "blue", lty=2)
abline(h=mean(df$ge), col = "green", lty=2)
plot.ts(df$ge)
abline(h=0, col = "red")
abline(h=mean(df$ge), col = "green", lty=2)
plot.ts(df[,4:7])  # рисуется линиями для временных рядов
plot.ts(df$ge)
data(CRSPday) #
plot.ts(df[,4:7])  # рисуется линиями для временных рядов
stats <- c(mean=mean(df$ge), sd=sd(df$ge), se.mean = sd(df$ge)/sqrt(nrow(df)-1))
stats
df[,4:7] <- df[,4:7] * 100
plot.ts(df[,4:7])  # рисуется линиями для временных рядов
plot.ts(df$ge)
abline(h=0, col = "red")
abline(h=mean(df$ge), col = "green", lty=2)
stats <- c(mean=mean(df$ge), sd=sd(df$ge), se.mean = sd(df$ge)/sqrt(nrow(df)-1))
stats
t.test(df$ge)
names(df)
print(var)
print(t.test(df[var]))
print(t.test(df[var]))
for (var in c("ge", "ibm", "mobil", "crsp")){
print(var)
print(t.test(df[var]))
}
colMeans((df[,4:7]))
cor(df[,4:7])
library(PerformanceAnalytics)
install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
chart.Correlation(df[,4:7])
chart.Correlation(df[4:7])
cor.test(df$ge, df$ibm)
t.test(scale(df$ge), scale(df$ibm))
t.test(scale(df$ge)*scale(df$ibm))
t.test(df$ge, df$ibm)
t.test(df$ge, df$ibm, paired = TRUE) # paired - спаренные, за один и тот же день берутся доходности
hist(df$ge, breaks = 20)
hist(df$ge, breaks = 100)
hist(df$ge, breaks = 100, freq = FALSE)
curve(dnorm(x, mean(df$ge), sd(df$ge), add = TRUE, col = "blue"))
curve(dnorm(x, mean(df$ge), sd(df$ge), add = TRUE, col = "blue"))
curve(dnorm(x, mean(df$ge), sd(df$ge)), add = TRUE, col = "blue")
stats <- c(asy=mean(scale(df$ge^3)))
stats <- c(asy=mean(scale(df$ge^3)), kurt=mean(scale(df$ge^3)))
stats
stats <- c(asy=mean(scale(df$ge)^3)), kurt=mean(scale(df$ge)^4)))
stats <- c(asy=mean(scale(df$ge)^3), kurt=mean(scale(df$ge)^4))
stats
t.test(scale(df$ge)^3)
t.test(scale(df$ge)^4)
t.test(scale(df$ge)^4-3) # куртозис
shapiro.test(df$ge)
qqnorm(df$ge)
qqline(df$ge)
n <- 83
set.seed(1542643545) # случайные числа, псевдослучайные
eps <- rnorm(n) # mean = 0, sd = 1
#qnorm()
#pnorm()
mean(eps)
var(eps)
x <- rnorm(n, mean=3, sd=2)
y <- -5 + 2 * x + eps
plot(y)
plot(y ~ x)
plot(y)
abline(h=0, col = "red")
abline(h=mean(df$ge), col = "green", lty=2)
abline(h=mean(y), col = "green", lty=2)
abline(h=mean(y), col = "green")
t.test(y)
cor.text(y,x)
cor.test(y,x)
n <- 83
set.seed(1542643545) # случайные числа, псевдослучайные
eps <- rnorm(n) # mean = 0, sd = 1
#qnorm()
#pnorm()
mean(eps)
var(eps)
x <- rnorm(n, mean=3, sd=2)
y <- -5 + 2 * x + eps
plot(y) # смотрим графически
abline(h=0, col = "red")
abline(h=mean(y), col = "green")
plot(y) # смотрим графически, среднее выше нуля
abline(h=0, col = "red")
abline(h=mean(y), col = "green")
t.test(y) # нулевую гипотезу отвергаем, среднее сстатитистически значимо больше 0
cor.test(y,x) # коэффициент корреляции значимо отличается от 0, нулевую гипотезу отвергаем
?t.test
t.test(y, x, var.equal = FALSE)
t.test(y, x, var.equal = TRUE)
t.test(y, x, var.equal = FALSE)
t.test(x, y, var.equal = FALSE)
t.test(y, x, var.equal = FALSE)
t.test(y, x, var.equal = FALSE, paired = TRUE)
t.test(scale(y)^3) # тест ассиметрии
t.test(scale(y)^4-3) # куртозис, значимо отличается от нуля
qqnorm(y) # квантиль нормального распределения сравнивается с нашими данными
qqline(y)
qqnorm(scale(y)) #
qqline(y)
qqnorm(y) #
qqline(y)
qqnorm(scale(y)) #
qqline(scale(y))
shapiro.test(y)
library(Ecdat)
data(CRSPday) #
df <- data.frame(CRSPday)
df[,4:7] <- df[,4:7] * 100
plot.ts(df[,4:7])  # рисуется линиями для временных рядов
plot.ts(CRSPday[,4:7])  # рисуется линиями для временных рядов
plot.ts(df[,4:7])  # рисуется линиями для временных рядов
View(df)
df <- CRSPday
df <- data.frame(CRSPday)
View(df)
names(df) <- CRSPday$year
names(df) <- df$year
View(df)
df$year
names(df) <- df$year
name(df) <- df$year
names(df) <- df$year
View(df)
names(df) <- 2:2529
names(df) <- 1:2528
View(df)
names(df)
library(Ecdat)
data(CRSPday) #
df <- data.frame(CRSPday)
df[,4:7] <- df[,4:7] * 100
plot.ts(df[,4:7])  # рисуется линиями для временных рядов
plot.ts(df$ge)
abline(h=0, col = "red")
abline(h=mean(df$ge), col = "green", lty=2)
stats <- c(mean=mean(df$ge), sd=sd(df$ge), se.mean = sd(df$ge)/sqrt(nrow(df)-1)) # за день
stats # колебания больше чем среднее, сильные колебания. se.mean - насколько точно измеряем среднее
t.test(df$ge) # p value - p значение, здесь маленькое число, нулевую гипотезу отвергаем для 5 процентов
for (var in c("ge", "ibm", "mobil", "crsp")){
print(var)
print(t.test(df[var]))
}
# names(df)
colMeans((df[,4:7])) # только среднее для 4 компаний
cor(df[,4:7])
library(PerformanceAnalytics)
chart.Correlation(df[4:7])
cor.test(df$ge, df$ibm) # тест корреляции по пирсону
t.test(scale(df$ge)*scale(df$ibm)) # произведение , scale - стандартизация сл. величины (делить на стандартную ошибку)
t.test(df$ge, df$ibm) # статастически разные, дисперсии там разные
t.test(df$ge, df$ibm, paired = TRUE) # paired - спаренные, за один и тот же день берутся доходности
hist(df$ge, breaks = 100, freq = FALSE) # freq = false - плотность а не в штуках
curve(dnorm(x, mean(df$ge), sd(df$ge)), add = TRUE, col = "blue")
stats <- c(asy=mean(scale(df$ge)^3), kurt=mean(scale(df$ge)^4))
stats
t.test(scale(df$ge)^3) # тест ассиметрии
t.test(scale(df$ge)^4-3) # куртозис, значимо отличается от нуля
shapiro.test(df$ge) # отклонили 0 гипотезу, там не t статистика, мощность у этого теста большая. Тест на нормальность
qqnorm(df$ge) # квантиль нормального распределения сравнивается с нашими данными
qqline(df$ge) # данные должны быть по линии ровной с наклоном 1. Данные отклоняются из-за выбросов.
chart.Correlation(df[4:7])
cor.test(df$ge, df$ibm) # тест корреляции по пирсону
t.test(scale(df$ge)*scale(df$ibm)) # произведение , scale - стандартизация сл. величины (делить на стандартную ошибку)
t.test(df$ge, df$ibm) # статастически разные, дисперсии там разные
t.test(df$ge, df$ibm, paired = TRUE) # paired - спаренные, за один и тот же день берутся доходности
hist(df$ge, breaks = 100, freq = FALSE) # freq = false - плотность а не в штуках
curve(dnorm(x, mean(df$ge), sd(df$ge)), add = TRUE, col = "blue")
shapiro.test(y)
hist(н, breaks = 100, freq = FALSE) # freq = false - плотность а не в штуках
hist(y, breaks = 100, freq = FALSE) # freq = false - плотность а не в штуках
hist(y, breaks = 20, freq = FALSE) # freq = false - плотность а не в штуках
hist(y, breaks = 10, freq = FALSE) # freq = false - плотность а не в штуках
hist(y, breaks = 20, freq = FALSE) # freq = false - плотность а не в штуках
hist(y, breaks = 25, freq = FALSE) # freq = false - плотность а не в штуках
hist(y, breaks = 30, freq = FALSE) # freq = false - плотность а не в штуках
hist(y, breaks = 10, freq = FALSE) # freq = false - плотность а не в штуках
hist(y, breaks = 15, freq = FALSE) # freq = false - плотность а не в штуках
hist(y, breaks = 8, freq = FALSE) # freq = false - плотность а не в штуках
hist(y, breaks = 10, freq = FALSE) # freq = false - плотность а не в штуках
curve(dnorm(y, mean(y), sd(y)), add = TRUE, col = "blue")
hist(y, breaks = 10, freq = FALSE) # freq = false - плотность а не в штуках
curve(dnorm(y, mean(y), sd(y)), add = TRUE, col = "blue")
curve(dnorm(x, mean(y), sd(y)), add = TRUE, col = "blue")
shapiro.test(y) # p - value = 0,76 > 0.05, распределение на уровне значимости 0.05 имеет нормальное распределение
plot(y) # смотрим графически, среднее выше нуля
abline(h=0, col = "red")
abline(h=mean(y), col = "green")
t.test(y) # нулевую гипотезу отвергаем, среднее сстатитистически значимо больше 0
cor.test(y,x) # коэффициент корреляции значимо отличается от 0, нулевую гипотезу отвергаем. Очень сильная корреляция, p значени 10^-16
t.test(y, x, var.equal = FALSE) # мат ожидания статистически отличаются друг от друга для непарных наблюдений
t.test(y, x, var.equal = FALSE, paired = TRUE) # мат ожидания статистически отличаются друг от друга для парных наблюдений
shapiro.test(y) # p - value = 0,76 > 0.05, распределение на уровне значимости 0.05 имеет нормальное распределение
hist(y, breaks = 10, freq = FALSE)
curve(dnorm(x, mean(y), sd(y)), add = TRUE, col = "blue")
hist(y, breaks = 15, freq = FALSE)
curve(dnorm(x, mean(y), sd(y)), add = TRUE, col = "blue")
hist(y, breaks = 5, freq = FALSE)
hist(y, breaks = 10, freq = FALSE)
curve(dnorm(x, mean(y), sd(y)), add = TRUE, col = "blue")
t.test(scale(y)^3) # ассиметрия статистически не отличается от 0, нулевую гипотезу принимаем
t.test(scale(y)^4-3) # куртозис статистически не отличается от 0, нулевую гипотезу принимаем
qqnorm(scale(y)) # графически квантиль распределения данных y лежит на линии нормальной кривой, имеются незначительные выбросы на концах
qqline(scale(y))
### Тема: Гетероскедостичность
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
df <- read.delim("StreetLight2020.tsv", skip=5) # read.table(sep="\t")
reg <- lm(actual ~ estimated, data=df)
summary(reg)
plot(actual ~ estimated, data=df) # есть гетероскедостичность
abline(reg, col="blue")
# График остатков (ei)
plot(resid(reg) ~ fitted(reg)) # resid - остатки для регрессии(e), fitted - расчетное значение (y с крышкой в качестве переменной z)
abline(h=0, col="blue") # есть гетероскед.
# Вспомогательная регрессия
reg2 <- lm(resid(reg)^2 ~ fitted(reg)) # квадраты остатков от расч. значения
# Бройша-Пейгана
summary(reg2) # F статистика, отклоняем гипотезу, гамма 1 не равна 0. Есть гетероскедостичность.
bptest(reg2)
# Сэндвич
library(sandwich)
library(lmtest)
# Бройша-Пейгана
summary(reg2) # F статистика, отклоняем гипотезу, гамма 1 не равна 0. Есть гетероскедостичность.
bptest(reg2)
plot(fitted(reg2)) # оценка для дисперсии, e^2. Дисперсия разная, большая гетероскедостичность.
coeftest(reg) # стандартные ошибки неправильные (здесь предположение о гомоскедостичности)
coeftest(reg, vcov=vcovHC) # vcov - ковариационная матрица, vcovHC - функция для расчета "правильной" ковариационной матрицы
confint(reg) # стандартный доверительный интервал
coefci(reg, vcov=vcovHC) # функция из пакета lmtest. С правильной ковариационной матрицы
reg <- lm(actual-estimated ~ estimated, data=df)
confint(reg) # стандартный доверительный интервал
coefci(reg, vcov=vcovHC) # нулевая гипотеза не откл., t статистика незначима
# Проверяем на нелинейность, b2E^2
reg <- lm(actual-estimated ~ estimated+I(estimated^2), data=df)
coeftest(reg)
coeftest(reg, vcov=vcovHC)
confint(reg) # стандартный доверительный интервал 95%. 1 для коэфф. наклона не попала в доверительный интервал.
coefci(reg, vcov=vcovHC) # функция из пакета lmtest. В 95% доверительный интервал значимо попала 1 с правильной ков. матрицей.
coeftest(reg)
coeftest(reg, vcov=vcovHC)
coeftest(reg)
coeftest(reg, vcov=vcovHC)
plot(fitted(reg2), col("blue")) # оценка для дисперсии, e^2. Наблюдаем визуально, что дисперсия разная, большая гетероскедостичность.
coeftest(reg, vcov=vcovHC)
reg.0 <- lm(actual-estimated ~ 0, data=df) # ничего нет в регрессии
# проверяем все три коэфф на равенство 0
anova(reg.0, reg) # неправильная F статистика, отклоняем гипотезу
waldtest(reg, reg.0, vcov=vcovHC) # значимо, но не столь жестко. Можно улучшить прогноз.
library(quantreg)
library(sandwich)
library(lmtest)
data(engel)
df <- data.frame(engel)
reg <- lm(foodexp ~ income, data=df)
coefci(reg, vcov=vcovHC)
plot(foodexp ~ income, data=df)
abline(reg, col="blue") # на графике видим увеличение разброса данных по мере увеличения переменной income
plot(resid(reg) ~ fitted(reg))
abline(h=0, col="blue") # квадраты остатков увеличиваются по мере увеличения расчетного значения income. Визуально есть гетероскедостичность.
# Вспомогательная регрессия
reg2 <- lm(resid(reg)^2 ~ fitted(reg)) # квадраты остатков от расч. значения
# Бройша-Пейгана
summary(reg2) # F статистика сильно значима, отклоняем гипотезу, гамма 1 не равна 0. Есть гетероскедостичность.
bptest(reg2)
plot(fitted(reg2))
# Логарифмическая регрессия
reg <- lm(log(foodexp) ~ log(income), data=df)
plot(log(foodexp) ~ log(income), data=df)
abline(reg, col="blue") # на графике не видим увеличения разброса данных по мере увеличения переменной income
plot(resid(reg) ~ fitted(reg))
abline(h=0, col="blue") # квадраты остатков не увеличиваются визуально по мере увеличения расчетного значения income. Визуально нет гетероскедостичность.
# Вспомогательная регрессия
reg2 <- lm(resid(reg)^2 ~ fitted(reg)) # квадраты остатков от расч. значения
# Бройша-Пейгана
summary(reg2) # F статистика p-value 0.00015. Нулевую гипотезу все равно отклоняем, но F статистика значительно меньше, 14 против 200.
plot(fitted(reg2))
bptest(reg2) # через хи квадрат.
plot(fitted(reg2))
bptest(reg2) # через хи квадрат.
# взвешенная дисперсия
wt <- 1/df$income ^ 2
reg.w <- lm(foodexp ~ income, weights = wt, data=df) # исправляем гетероскедостичность.
coefci(reg.w, vcov=vcovHC)
plot(rstandard(reg.w) ~ fitted(reg.w)) # нормированные остатки от расчетных значений
reg2.w <- lm(rstandard(reg.w)^2 ~ fitted(reg.w))
bptest(reg2.w)
summary(reg2.w)
