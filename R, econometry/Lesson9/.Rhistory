setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
df <- read.csv("smetana.csv", stringsAsFactors=FALSE)
lct <- Sys.getlocale("LC_TIME"); Sys.setlocale("LC_TIME", "C") # сохраняем текущую и устанавливаем "C"
df$period <- as.Date(df$period, "%d %b %Y")
Sys.setlocale("LC_TIME", lct)
df <- df[order(df$period), c("period", "value")] # сортировка
p <- df$value # создаем вектор цен p на товар
View(df)
names(p)=df$period
View(p)
#график
#Построить график (линии и маркеры) ряда цен от даты (команда plot(), pch=вид маркера, cex=размер маркера). Добавить линию нуля (команда abline() с опцией h=)
plot(p ~ as.Date(names(p)), type= "b", pch=10, cex=1)
lines(p ~ as.Date(names(p)))
abline(h=0, col = "blue") # цены выше нуля
#Конвертировать ряда dp во помесячный временной ряд (команда ts() с опциями frequency и start=c(год,месяц)). Построить график (команда plot() или plot.ts()).
p.ts=ts(p, frequency = 12, start=c(2017,01))
plot(p.ts)
#Конвертировать ряда dp во помесячный временной ряд (команда ts() с опциями frequency и start=c(год,месяц)). Построить график (команда plot() или plot.ts()).
p.ts=ts(p, frequency = 12, start=c(2017,01))
#график
#Построить график (линии и маркеры) ряда цен от даты (команда plot(), pch=вид маркера, cex=размер маркера). Добавить линию нуля (команда abline() с опцией h=)
plot(p ~ as.Date(names(p)), type= "b", pch=10, cex=1)
lines(p ~ as.Date(names(p)))
abline(h=0, col = "blue") # цены выше нуля
#Конвертировать ряда dp во помесячный временной ряд (команда ts() с опциями frequency и start=c(год,месяц)). Построить график (команда plot() или plot.ts()).
p.ts=ts(p, frequency = 12, start=c(2017,01))
plot(p.ts)
View(p.ts)
DP=((diff(p)/p[-length(p)]+1)^12-1)*100
plot(DP ~ as.Date(names(DP)), type= "b", pch=10, cex=1)
abline(h=0, col = "red")
library(psych)
describe(dp)
#Создать ряды логарифмических темпов прироста (dp) и обычных темпов прироста (DP) в % в годовом исчислении (умножить на 1200 % - это значит на сколько поменяется помесячно, но в годовом исчислении). Построить аналогичный график. Были ли в цене вашего товара большие скачки?
dp=diff(log(p))*100*12
plot(dp ~ as.Date(names(dp)), type= "b", pch=10, cex=1)
lines(dp ~ as.Date(names(dp)))
abline(h=0, col = "red")
DP=((diff(p)/p[-length(p)]+1)^12-1)*100
abline(h=0, col = "red")
plot(DP ~ as.Date(names(DP)), type= "b", pch=10, cex=1)
library(psych)
describe(dp)
describe(DP)
#Построить гистограмму ряда dp, подобрав подходящую ширину интервала (команда hist(), опция breaks=. для полученя плотности, а не частоты опция freq=FALSE). Добавить «бахому» наблюдений (командой rug()). Добавить нормальную кривую с соответствующими dp параметрами mean и sd. (Для кривой функции команда curve() с опцией add=TRUE. Функция плотности нормального распределения dnorm()).
hist(dp, breaks = 25, freq = FALSE)
rug(dp)
curve(dnorm(x, mean(dp), sd(dp)), add=TRUE, col="red")
install.packages("Ecdat")
library(Ecdat)
data(CRSPday) #
df <- data.frame(CRSPday)
df[,4:7] <- df[,4:7] * 100
rm(CRSPday) # удалить данные
plot.ts(df[,4:7])  # рисуется линиями для временных рядов
plot.ts(df$ge)
plot.ts(df$ge)
abline(h=0, col = "red")
abline(h=mean(df$ge), col = "green", lty=2)
stats <- c(mean=mean(df$ge), sd=sd(df$ge), se.mean = sd(df$ge)/sqrt(nrow(df)-1)) # за день
stats # колебания больше чем среднее, сильные колебания. se.mean - насколько точно измеряем среднее
cor(df[,4:7])
library(PerformanceAnalytics)
chart.Correlation(df[4:7])
cor.test(df$ge, df$ibm) # тест корреляции по пирсону
t.test(scale(df$ge)*scale(df$ibm)) # произведение , scale - стандартизация сл. величины (делить на стандартную ошибку)
cor.test(df$ge, df$ibm) # тест корреляции по пирсону
t.test(scale(df$ge)*scale(df$ibm)) # произведение , scale - стандартизация сл. величины (делить на стандартную ошибку)
hist(df$ge, breaks = 100, freq = FALSE) # freq = false - плотность а не в штуках
curve(dnorm(x, mean(df$ge), sd(df$ge)), add = TRUE, col = "blue")
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
df <- read.delim("temp_wage.tsv", skip=3)
names(df)[1] <- "region" # меняем первую переменную на region
plot(wage ~ temp, data=df)
cor(df$wage, df$temp)
# регрессия, получается список всяких характеристик
reg <- lm(wage ~ temp, data=df) # wage - зависимая часть, temp - обьясняющая
# lm(df$wage~df$temp) - так тоже можно
summary(reg) # intercept - константа (31,9), temp - обьясняющая пер.
# получился отрицательный наклон -1,2353
# wage = 31.9079 - 1.2353 * temp + остатки
plot(wage ~ temp, data=df)
abline(reg, col="blue", lw=2) # рисуем линию регрессии, lw - толщина линии
cor(df$wage, df$temp)^2 # 0.25906, reg - multiple R-squared = 0,2591 = 25,91%
# найдем расчетные значения и положим их на график
plot(wage ~ fitted(reg), data=df, asp=1) # по горизонтали - расчетные значения, asp - шаг по y и по x равный
abline(c(0,1), col="blue", lw=2) # добавляем линию с наклоном 1 - 100 процентов если совпадают
cor(df$wage, fitted(reg))^2 # 0.259 между y и y^
# найдем расчетные значения и положим их на график
plot(wage ~ fitted(reg), data=df, asp=1) # по горизонтали - расчетные значения, asp - шаг по y и по x равный
abline(c(0,1), col="blue", lw=2) # добавляем линию с наклоном 1 - 100 процентов если совпадают
cor(df$wage, fitted(reg))^2 # 0.259 между y и y^
# обратная регрессия
reg.inv = lm(temp ~ wage, data=df)
summary(reg.inv) # R^2 такой же, но коэф. другие
# temp = 0.31403 - 0.20971 * wage + остаток
# wage = 0.31403 / 0.20971 - 1 / 0.20971 * temp
cf.inv <- coef(reg.inv) # получем коэф. регрессии
cf.inv
# Данные
plot(wage ~ temp, data=df)
# прямая регрессия (исходная)
abline(reg, col="blue", lw=2)
# Обратная регрессия
abline(c(-cf.inv[1]/cf.inv[2], 1/cf.inv[2]), col="red", lw=2)
# две линии средних
abline(v=mean(df$temp))
abline(h=mean(df$wage))
# Задание новое
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
df <- read.delim("StreetLight2020.tsv", skip=5)
plot(df$actual ~ df$estimated, asp=1)
abline(c(0,1), col="blue", lw=2, lty=2) # теоритически правильная линия регрессии
reg <- lm(actual ~ estimated, data=df) # actual от estimated
summary(reg) # стандартная ошибка оценки (error). t статистика для гипотезы, что коэфф равен 0
# Доверительный интервал для коэффициентов
confint(reg) # 95% , est не попала 1, значит прогноз не очень хороший, коэфф значимо отличается от 1. константа 0 попадает. Не учли тут гетероскедостичность!
reg <- lm(actual-estimated ~ estimated, data=df)
summary(reg) # нулевая гипотеза для коэф наклона отклоняется, он значимо отличается от 0. (p-value 0,00145). Прогноз можно улучшить.
# регрессия от константы (только константа)
reg <- lm(actual-estimated ~ 1, data=df) # тест на смещенность прогноза.
summary(reg) # на уровне 5 процентов значима константа отличается от 0
t.test(df$actual - df$estimated) # константа значимо отличается от 0, наблюдения спарены (идут парами)
t.test(df$actual, df$estimated, paired=TRUE)
# коэффициенты β0 = 0, β1 = 1, и β2 = 0 - нулевая гипотеза
# Добавьте в регрессию actual от estimated квадрат estimated. Проверяем нелинейность.
reg <- lm(actual ~ estimated + I(estimated^2), data=df)
summary(reg) # нулевая гипотеза - квадрат не нужен. p значение 10^-6 - нулевую гипотезу отвергаем. Квадрат нужен.
# Постройте регрессию actual–estimated от estimated и квадрата estimated (и константы).
reg1 <- lm(actual-estimated ~ estimated + I(estimated^2), data=df) # I - упаковываем формулу в переменную
summary(reg1) # F-statistic (k=2, n-k-1) - p-value 10^-7 нулевую гипотезу отклоняем (нулевая гипотеза – обе переменные не нужны в регрессии, т. е. регрессия в целом не значима, константу не трогаем). Хотя бы одна переменная нужна!!
# регрессия от константы
reg.const <- lm(actual-estimated ~ 1, data=df)
summary(reg2)
# регрессия от константы
reg.const <- lm(actual-estimated ~ 1, data=df)
summary(reg.const)
t.test(df$actual - df$estimated) # константа значимо отличается от 0, наблюдения спарены (идут парами)
# Повторите предыдущий тест с помощью команды anova(регрессия1, регрессия2).
anova(reg.const, reg1) # не важно в каком порядке, меняется знак остатков, RSS - сумма квадратов, 1045 - разница на которую они отличаются. Там не нули в каком-то коэфф.
anova(reg.0, reg1) # тест на равенство нулю всех 3 коэффициентов.
reg.0 <- lm(actual-estimated ~ 0, data=df)
anova(reg.0, reg1) # тест на равенство нулю всех 3 коэффициентов.
summary(reg.0)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
df <- read.delim("StreetLight2020.tsv", skip=5) # actual - траффик машин, actual в тысячах штук
# Оценка плотности для actual
x <- df$actual
summary(x) # сильно не симметричное распределение, есть выбросы
hist(x, breaks = 200, freq=FALSE)
hist(log(x), breaks = 100, freq=FALSE)
hist(x, breaks = 200, freq=FALSE)
hist(log(x), breaks = 100, freq=FALSE)
# ядерная оценка
plot(density(x, adjust=0.1)) # kernal density estimator, adjust - h полоса
#kernel = c("gaussian", "epanechnikov", "rectangular",
#           "triangular", "biweight",
#           "cosine", "optcosine")
?density
# ядерная оценка
plot(density(x, adjust=0.1)) # kernal density estimator, adjust - h полоса
# ядерная оценка
plot(density(x, adjust=0.5)) # kernal density estimator, adjust - h полоса
# ядерная оценка
plot(density(x, adjust=0.1)) # kernal density estimator, adjust - h полоса
plot(density(log(x), adjust=0.3))
bounds <- c(0.1, 0.33, 1, 3.3, 10, 33, 100, 330) # посчитаем чистоты для этих интервалов. В логарифмах это примерное одномерный шаг
hi <- hist(x, breaks=bounds, plot=FALSE)
df2 <- data.frame(counts = hi$counts)
prop
df2$perc <- prop.table(df2$counts) * 100
row.names(df2) <- paste0(bounds[1:7], "_", bounds[2:8])
df2$lab.perc <- paste0(round(df2$perc, 2), "%")
df2
View(hi)
bounds <- c(0.1, 0.33, 1, 3.3, 10, 33, 100, 330) # посчитаем чистоты для этих интервалов. В логарифмах это примерное одномерный шаг
hi <- hist(x, breaks=bounds, plot=FALSE)
hi$counts # частоты в штуках
### Тема: Гетероскедостичность
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
df <- read.delim("StreetLight2020.tsv", skip=5) # read.table(sep="\t")
reg <- lm(actual ~ estimated, data=df)
summary(reg)
plot(actual ~ estimated, data=df) # Визуально на графике наблюдаем существенное увеличение разброса ошибок по мере возрастания estimated.
abline(reg, col="blue")
# График остатков (ei)
plot(resid(reg) ~ fitted(reg)) # resid - остатки для регрессии(e), fitted - расчетное значение (y с крышкой в качестве переменной z)
abline(h=0, col="blue") # разброс остатков регрессии визуально увеличивается при увеличении расчетного значения.
# Вспомогательная регрессия
reg2 <- lm(resid(reg)^2 ~ fitted(reg)) # квадраты остатков от расч. значения
# Бройша-Пейгана
summary(reg2) # F статистика 563 (p-value 2*10^-16), отклоняем гипотезу о гомоскедостичности при уровне значимости 10^-15, гамма 1 не равна 0.
plot(fitted(reg2)) # оценка для дисперсии, e^2. Наблюдаем визуально, что дисперсия разная, большая гетероскедостичность.
# Вспомогательная регрессия
reg2 <- lm(resid(reg)^2 ~ fitted(reg)) # квадраты остатков от расч. значения
# График остатков (ei)
plot(resid(reg) ~ fitted(reg)) # resid - остатки для регрессии(e), fitted - расчетное значение (y с крышкой в качестве переменной z)
abline(h=0, col="blue") # разброс остатков регрессии визуально увеличивается при увеличении расчетного значения.
plot(fitted(reg2)) # оценка для дисперсии, e^2. Наблюдаем визуально, что дисперсия разная, большая гетероскедостичность.
# Сэндвич
library(sandwich)
library(lmtest)
coeftest(reg) # стандартные ошибки неправильные (здесь предположение о гомоскедостичности)
coeftest(reg, vcov=vcovHC) # vcov - ковариационная матрица, vcovHC - функция для расчета "правильной" ковариационной матрицы
confint(reg) # стандартный доверительный интервал 95%. 1 для коэфф. наклона не попала в доверительный интервал.
coefci(reg, vcov=vcovHC) # функция из пакета lmtest. В 95% доверительный интервал значимо попала 1 с правильной ков. матрицей.
reg <- lm(actual-estimated ~ estimated, data=df) # A-E=b0+b1*E+e
confint(reg) # стандартный доверительный интервал. 0 не попал для коэф. наклона. 0 для константы попала.
coefci(reg, vcov=vcovHC) # нулевая гипотеза не откл. о b1=0, t статистика незначима
# Проверяем на нелинейность, b2E^2
reg <- lm(actual-estimated ~ estimated+I(estimated^2), data=df)
coeftest(reg) # t-test показал, что b1 и b2 по отдельности значимо отличаются от нуля для уровня значимости 0.01.
coeftest(reg, vcov=vcovHC) # b1 и b2 незначимы для 5%. p-value 0,16 - b1, 0,12 - b2.
reg.0 <- lm(actual-estimated ~ 0, data=df) # ничего нет в регрессии (b0=0, b1=0, b2=0)
# проверяем все три коэфф на равенство 0
anova(reg.0, reg) # F статистика значима (p-value 4*10^-8), отклоняем нулевую гипотезу.
waldtest(reg, reg.0, vcov=vcovHC) # значимо(отклоняем нулевую гипотезу), но с правильной ков. матрицей F статистика значительно меньше, p-value 0,0011. Можно улучшить прогноз.
library(quantreg)
library(sandwich)
library(lmtest)
data(engel)
df <- data.frame(engel)
reg <- lm(foodexp ~ income, data=df)
coefci(reg, vcov=vcovHC) # 0.35-0.61 для коэффициента наклона 95% дов. интервал.
plot(foodexp ~ income, data=df)
abline(reg, col="blue") # на графике видим увеличение разброса данных по мере увеличения переменной income. То есть гомоскедостичность визуально нарушается.
plot(resid(reg) ~ fitted(reg))
abline(h=0, col="blue") # квадраты остатков увеличиваются по мере увеличения расчетного значения income. Визуально есть гетероскедостичность.
# Вспомогательная регрессия
reg2 <- lm(resid(reg)^2 ~ fitted(reg)) # квадраты остатков от расч. значения
# Бройша-Пейгана
summary(reg2) # F статистика сильно значима (p-value 2.2*10^-16), отклоняем нулевую гипотезу, гамма 1 не равна 0. Есть гетероскедостичность.
bptest(reg2) # p-value 2.2*10^-16, отклоняем нулевую гипотезу.
plot(fitted(reg2)) # видны визуальные отклонения в оценке дисперсии по мере изменений расчетного значения.
# Логарифмическая регрессия
reg <- lm(log(foodexp) ~ log(income), data=df)
plot(log(foodexp) ~ log(income), data=df)
abline(reg, col="blue") # на графике не видим увеличения разброса данных по мере увеличения переменной income. С логарифмом значительно сгладили отклонения.
plot(resid(reg) ~ fitted(reg))
abline(h=0, col="blue") # квадраты остатков не увеличиваются визуально по мере увеличения расчетного значения income. Визуально нет гетероскедостичность.
# Вспомогательная регрессия
reg2 <- lm(resid(reg)^2 ~ fitted(reg)) # квадраты остатков от расч. значения
# Бройша-Пейгана
summary(reg2) # F статистика p-value 0.00015. Нулевую гипотезу все равно отклоняем, но F статистика значительно меньше, 14 против 200.
plot(fitted(reg2))
bptest(reg2) # через хи квадрат.p-value 0.00355. Нулевую гипотезу отклоняем.
# взвешенная дисперсия
wt <- 1/df$income ^ 2
reg.w <- lm(foodexp ~ income, weights = wt, data=df) # исправляем гетероскедостичность.
coefci(reg.w, vcov=vcovHC) # 0.54-0.50 - значительно улучшили точность довер. интервала.
plot(rstandard(reg.w) ~ fitted(reg.w)) # нормированные остатки от расчетных значений
reg2.w <- lm(rstandard(reg.w)^2 ~ fitted(reg.w))
bptest(reg2.w) # p-value 0.013 - для уровня значимости 0.01 нулевую гипотезу не отклоняем.
summary(reg2.w)
# Медианная регрессия (и квантильные)
qreg <- rq(foodexp ~ income, data=df) # квантильная регрессия
qreg90 <- rq(foodexp ~ income, data=df, tau=0.9)
qreg10 <- rq(foodexp ~ income, data=df, tau=0.1)
plot(foodexp ~ income, data=df)
abline(qreg)
abline(qreg90)
abline(qreg10)
# Логарифмическая регрессия
reg <- lm(log(foodexp) ~ log(income), data=df)
plot(log(foodexp) ~ log(income), data=df)
abline(reg, col="blue") # на графике не видим увеличения разброса данных по мере увеличения переменной income. С логарифмом значительно сгладили отклонения.
plot(fitted(reg2)) # видны визуальные отклонения в оценке дисперсии по мере изменений расчетного значения.
plot(foodexp ~ income, data=df)
abline(reg, col="blue") # на графике видим увеличение разброса данных по мере увеличения переменной income. То есть гомоскедостичность визуально нарушается.
plot(resid(reg) ~ fitted(reg))
reg <- lm(foodexp ~ income, data=df)
coefci(reg, vcov=vcovHC) # 0.35-0.61 для коэффициента наклона 95% дов. интервал.
plot(foodexp ~ income, data=df)
abline(reg, col="blue") # на графике видим увеличение разброса данных по мере увеличения переменной income. То есть гомоскедостичность визуально нарушается.
plot(resid(reg) ~ fitted(reg))
abline(h=0, col="blue") # квадраты остатков увеличиваются по мере увеличения расчетного значения income. Визуально есть гетероскедостичность.
plot(foodexp ~ income, data=df)
abline(reg, col="blue") # на графике видим увеличение разброса данных по мере увеличения переменной income. То есть гомоскедостичность визуально нарушается.
# Бройша-Пейгана
summary(reg2) # F статистика p-value 0.00015. Нулевую гипотезу все равно отклоняем, но F статистика значительно меньше, 14 против 200.
plot(fitted(reg2))
bptest(reg2) # через хи квадрат.p-value 0.00355. Нулевую гипотезу отклоняем.
plot(resid(reg) ~ fitted(reg))
abline(h=0, col="blue") # квадраты остатков не увеличиваются визуально по мере увеличения расчетного значения income. Визуально нет гетероскедостичность.
# Логарифмическая регрессия
reg <- lm(log(foodexp) ~ log(income), data=df)
plot(log(foodexp) ~ log(income), data=df)
abline(reg, col="blue") # на графике не видим увеличения разброса данных по мере увеличения переменной income. С логарифмом значительно сгладили отклонения.
plot(resid(reg) ~ fitted(reg))
# Логарифмическая регрессия
reg <- lm(log(foodexp) ~ log(income), data=df)
plot(log(foodexp) ~ log(income), data=df)
abline(reg, col="blue") # на графике не видим увеличения разброса данных по мере увеличения переменной income. С логарифмом значительно сгладили отклонения.
TT <- 100
# Белый шум
set.seed(123) # задаем начальное число для датчика
eps <- rnorm(TT, sd=2)
#plot(eps, type = "l")
#plot(eps, type = "b")
plot.ts(eps) # график для временных рядов
#plot(eps, type = "l")
#plot(eps, type = "b")
plot(eps) # график для временных рядов
eps <- rnorm(TT, sd=2)
#plot(eps, type = "l")
#plot(eps, type = "b")
plot.ts(eps) # график для временных рядов
#plot(eps, type = "l")
#plot(eps, type = "b")
plot(eps) # график для временных рядов
#plot(eps, type = "l")
#plot(eps, type = "b")
plot(eps, type="l") # график для временных рядов
#plot(eps, type = "l")
#plot(eps, type = "b")
plot.ts(eps) # график для временных рядов
abline(h=0, col="blue")
acf(eps) # АКФ для белого шума, случайно болтается без какой то системы
# AR - авторегрессия
phi <- 0.8
x <- eps[1] # t=1
for (t in 2:TT) {
x[t] <- phi * x[t-1] + eps[t]
}
#x <- x + 10 # если хотим среднее 10
plot.ts(x) # график для временных рядов
abline(h=0, col="blue")
acf(x, lag.max=90) # убывает, но случайно уходит в отриц. область
phi <- -0.8 # орицательная корреляция между соседними наблюдениями
x <- eps[1] # t=1
for (t in 2:TT) {
x[t] <- phi * x[t-1] + eps[t]
}
plot.ts(x) # график для временных рядов
abline(h=0, col="blue")
acf(x, lag.max=30)
phi <- 1 # случайное блуждание
x <- eps[1] # t=1
for (t in 2:TT) {
x[t] <- phi * x[t-1] + eps[t]
}
plot.ts(x) # график для временных рядов
abline(h=0, col="blue")
acf(x, lag.max=20)
# задание с курсом доллара
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
df <- read.csv("usd_volume.txt")
df <- data.frame(date=df$X.DATE., x = df$X.VOL.)
df$date <- as.Date(as.character(df$date), "%Y%m%d") # as.character преобразует в строчку числа
df <- df[df$x != 0, ]
# Лаги будем создавать внутри фрейма
TT <- nrow(df)
df <- within(df, # within - внутри датафрейма работаем.
{x1 <- c(x[1], x[-TT]) # 1-й лаг
x2 <- c(x1[1], x1[-TT]) # второй
x3 <- c(x2[1], x2[-TT])
x4 <- c(x3[1], x3[-TT])}
)
min(df$x)
plot(df$x ~ df$date, type="l")
acf(df$x) # автокорреляционная функция не угасает, медленно убывает. Похоже на нестационарность.
acf(df$x)[1] # первая автокорреляция
# Авторегрессия 1 порядка
reg1 <- lm(x ~ x1, data=df)
summary(reg1) # коэф. автокорреляции 0.688
reg2 <- lm(x ~ x1+x2, data=df)
summary(reg2)
reg3 <- lm(x ~ x1+x2+x3, data=df)
summary(reg3) # коэф. автокорреляции 0.688
reg4 <- lm(x ~ x1+x2+x3+x4, data=df)
summary(reg4)
library(lmtest)
library(sandwich) # ковар. матрица
# Авторегрессия 1 порядка
reg1 <- lm(x ~ x1, data=df)
summary(reg1) # коэф. автокорреляции 0.688
reg2 <- lm(x ~ x1+x2, data=df)
summary(reg2)
reg3 <- lm(x ~ x1+x2+x3, data=df)
summary(reg3) # коэф. автокорреляции 0.688
reg4 <- lm(x ~ x1+x2+x3+x4, data=df)
summary(reg4)
library(lmtest)
library(sandwich) # ковар. матрица
coeftest(reg4, vcov=vcovHAC) # все коэф. значимы, все лаги нужны, укоротить нельзя.
# тест бройша-годфри
bgtest(reg4) # LM-test, 1 лаг был добавлен, p значение маленькое, 0 гипотезу отклоняем
bgtest(reg4, order=5) # жестко отклоняем
acf(df$x) # автокорреляционная функция не угасает, медленно убывает. Похоже на нестационарность.
acf(df$x)[1] # первая автокорреляция
acf(df$x) # автокорреляционная функция не угасает, медленно убывает. Похоже на нестационарность.
plot(df$x ~ df$date, type="l")
acf(df$x) # автокорреляционная функция не угасает, медленно убывает. Похоже на нестационарность.
# Авторегрессия 1 порядка
reg1 <- lm(x ~ x1, data=df)
summary(reg1) # коэф. автокорреляции 0.688
# Лаги будем создавать внутри фрейма
TT <- nrow(df)
df <- within(df, # within - внутри датафрейма работаем.
{x1 <- c(x[1], x[-TT]) # 1-й лаг
x2 <- c(x1[1], x1[-TT]) # второй
x3 <- c(x2[1], x2[-TT])
x4 <- c(x3[1], x3[-TT])}
)
min(df$x)
plot(df$x ~ df$date, type="l")
acf(df$x) # автокорреляционная функция не угасает, медленно убывает. Похоже на нестационарность.
acf(df$x)[1] # первая автокорреляция
plot(df$x ~ df$date, type="l")
acf(df$x) # автокорреляционная функция не угасает, медленно убывает. Похоже на нестационарность.
acf(df$x)[1] # первая автокорреляция
# Прогнозирование
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
df <- read.delim("Exchange_Rate_Report.tsv", skip=0, sep="")
Sys.setlocale("LC_TIME", "C")
df$Date <- as.Date(df$Date, "%d-%b-%Y") # преобразовываем дату
x <- df$Swiss_franc
plot.ts(x)
# 1 задание: Разделить на ряды по 20 наблюдений и создать из них матрицу (matrix(x, nrow=20))
XX <- matrix(x, nrow=20)
XX <- XX[, -24]
# Рассчитать матрицу корреляций.
RR <- cor(XX)
rr <- c(RR[lower.tri(RR)]) # нижний треугольник
hist(rr) # получили много ложных корреляций и на гистограмме это хорошо видно
# Задание 2. Многошаговое прогнозирование валютного курса с помощью тренда
train <- df$Date < "2020-01-03" # На тех же данных. Тренировочный период 1 год – построить линейный тренд. Нарисовать.
test <- ! train
tr <- 1:nrow(df)
tr <- 1:nrow(df)
reg1 <- lm(x ~ tr, subset=train) # линейная регрессия на первом году
plot.ts(x[train])
lines(fitted(reg1), col="blue") # расчетное значение
xp <- predict(reg1, newdata=list(tr=tr[test])) # получаем прогноз
plot.ts(x[test])
lines(xp, col="blue", lw=2)
tr2 <- tr^2 # сделаем для квадратичного тренда
reg2 <- lm(x ~ tr+tr2, subset=train) # линейная регрессия на первом году
plot.ts(x[train])
lines(fitted(reg2), col="blue")
xp2 <- predict(reg2, newdata=list(tr=tr[test], tr2=tr2[test])) # получаем прогноз
plot.ts(x[test])
lines(xp2, col="blue", lw=2)
# Задание 3.
x1 <- c(x[1], x[-length(x)]) #первый лаг
x2 <- c(x1[1], x1[-length(x)])
AR1 <- lm(x ~ x1, subset=train)
cfAR1 <- coef(AR1)
AR2 <- lm(x ~ x1+x2, subset=train)
cfAR2 <- coef(AR2)
AR02 <- lm(x ~ x2, subset=train)
cfAR02 <- coef(AR02)
xpRW <- x1[test]
reg11 <- lm(epep[,4]^2 - epep[,1]^2 ~ 1)
library(lmtest)
library(sandwich)
reg11 <- lm(epep[,4]^2 - epep[,1]^2 ~ 1)
# ошибки прогноза
epep <- x[test] - xpxp
xpRW <- x1[test]
xpAR1 <- cfAR1[1] + cfAR1[2] * x1[test]
xpAR2 <- cfAR2[1] + cfAR2[2] * x1[test] + cfAR2[3] * x2[test]
xpAR02 <- cfAR02[1] + cfAR02[2] * x2[test]
xpxp <- cbind(xpRW, xpAR1, xpAR2, xpAR02) # прогнозные значения
ts.plot(xpxp, col=1:4)
# ошибки прогноза
epep <- x[test] - xpxp
ts.plot(epep, col=1:4)
abline(h=0)
MSE <- colMeans(epep^2) # среднее по столбцам. Получаем средний квадрат ошибки
RMSE <- sqrt(MSE)
RMSE
# Значимость, 1 задание
epep[,1]
library(lmtest)
library(sandwich)
reg11 <- lm(epep[,4]^2 - epep[,1]^2 ~ 1)
lmtest::coeftest(reg11, vcov=vcovHAC) # прогноз на основе предыдующего значения значим лучше, чем прогноз со 2 лагом. p-value 10^-10
